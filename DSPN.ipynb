{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T14:12:08.664739Z",
     "start_time": "2025-03-09T14:12:02.696193Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from preprocess import TripPreProcess, ASAPPreProcess\n",
    "from models import DSPN\n",
    "from utils import set_seed\n",
    "from trainer import DSPN_trainer\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import AlbertTokenizer, AlbertModel\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T14:12:15.137872Z",
     "start_time": "2025-03-09T14:12:15.124557Z"
    }
   },
   "outputs": [],
   "source": [
    "data_name = 'Trip' # ['ASAP', 'Trip', 'rest_14', 'rest_15', 'rest_16', 'mams']\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "seed = 1\n",
    "set_seed(seed)\n",
    "n_epochs = 5\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./model_params/bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at ./model_params/bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "if data_name == 'Trip':\n",
    "    trip = TripPreProcess()\n",
    "    T, train_set, dev_set, test_set = trip.get_dataset()\n",
    "    bert_model = BertModel.from_pretrained(\"./model_params/bert-base-uncased\")\n",
    "    bert_tokenizer = BertTokenizer.from_pretrained(\"./model_params/bert-base-uncased\")\n",
    "    #bert_model = RobertaModel.from_pretrained(\"./model_params/roberta-base\")\n",
    "    #bert_tokenizer = RobertaTokenizer.from_pretrained(\"./model_params/roberta-base\")\n",
    "    #bert_model = AlbertModel.from_pretrained(\"./model_params/albert-base-v2\")\n",
    "    #bert_tokenizer = AlbertTokenizer.from_pretrained('./model_params/albert-base-v2')\n",
    "elif data_name == 'ASAP':\n",
    "    asap = ASAPPreProcess()\n",
    "    T, train_set, dev_set, test_set = asap.get_dataset()\n",
    "    bert_model = BertModel.from_pretrained(\"./model_params/bert-base-chinese\")\n",
    "    bert_tokenizer = BertTokenizer.from_pretrained(\"./model_params/bert-base-chinese\")\n",
    "    #bert_model = AutoModel.from_pretrained(\"./model_params/roberta-base-chinese\")\n",
    "    #bert_tokenizer = AutoTokenizer.from_pretrained(\"./model_params/roberta-base-chinese\")\n",
    "    #bert_model = AlbertModel.from_pretrained(\"./model_params/albert-chinese-base/\")\n",
    "    #bert_tokenizer = AutoTokenizer.from_pretrained(\"./model_params/albert-chinese-base/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DSPN(T, bert_model, bert_tokenizer).to(device)\n",
    "trainer = DSPN_trainer(data_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "trainer.train(model, train_set, dev_set, device=device, n_epochs=n_epochs, batch_size=batch_size, data_name=data_name, model_name='DSPN_'+str(seed))\n",
    "end_time = time.time()\n",
    "used_mins = (end_time - start_time) / 60\n",
    "print(f\"Time: {used_mins} Minutes\")\n",
    "num_params = sum(p.numel() for p in model.state_dict().values())\n",
    "print(f\"Params_size: {num_params/1000000}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"./model_params/\" + data_name + \"_DSPN_\"+ str(seed) +\".model\", map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.6703865569383266\n",
      "Recall: 0.6638848513096409\n",
      "F1-score: 0.6593709032195875\n",
      "Accuracy: 0.7228708791208791\n"
     ]
    }
   ],
   "source": [
    "trainer.test_rp(model, test_set, batch_size, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Th: 0.00000000 | P: 0.86455063 | R: 1.00000000 | F1: 0.92735549\n",
      "Th: 0.00052632 | P: 0.86729645 | R: 0.87521988 | F1: 0.87124015\n",
      "Th: 0.00105263 | P: 0.86375141 | R: 0.69715712 | F1: 0.77156404\n",
      "Th: 0.00157895 | P: 0.85492979 | R: 0.63570334 | F1: 0.72919582\n",
      "Th: 0.00210526 | P: 0.85355550 | R: 0.61913409 | F1: 0.71768730\n",
      "Th: 0.00263158 | P: 0.85053352 | R: 0.59252114 | F1: 0.69846154\n",
      "Th: 0.00315789 | P: 0.85993038 | R: 0.53265619 | F1: 0.65783664\n",
      "Th: 0.00368421 | P: 0.85979401 | R: 0.50212790 | F1: 0.63399606\n",
      "Th: 0.00421053 | P: 0.85656836 | R: 0.47137264 | F1: 0.60810366\n",
      "Th: 0.00473684 | P: 0.85616512 | R: 0.45429269 | F1: 0.59360866\n",
      "Th: 0.00526316 | P: 0.85369128 | R: 0.43307042 | F1: 0.57463389\n",
      "Th: 0.00578947 | P: 0.85343635 | R: 0.41996255 | F1: 0.56292071\n",
      "Th: 0.00631579 | P: 0.85668677 | R: 0.41111048 | F1: 0.55559816\n",
      "Th: 0.00684211 | P: 0.85944505 | R: 0.40248539 | F1: 0.54823002\n",
      "Th: 0.00736842 | P: 0.86084063 | R: 0.39630029 | F1: 0.54274168\n",
      "Th: 0.00789474 | P: 0.86151731 | R: 0.39113658 | F1: 0.53801124\n",
      "Th: 0.00842105 | P: 0.86170617 | R: 0.38574590 | F1: 0.53292568\n",
      "Th: 0.00894737 | P: 0.86161344 | R: 0.38120638 | F1: 0.52856019\n",
      "Th: 0.00947368 | P: 0.86236503 | R: 0.37615616 | F1: 0.52382458\n",
      "Th: 0.01000000 | P: 0.86256112 | R: 0.37036827 | F1: 0.51822152\n"
     ]
    }
   ],
   "source": [
    "trainer.test_acd(model, test_set, batch_size, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: 0.43828 | R: 0.50695 | F1: 0.47012\n",
      "ACSA Accuracy: 0.506951143392158\n",
      "      p=-1   p=0   p=1\n",
      "t=-1  3656   704  1023\n",
      "t=0   1817   687  1813\n",
      "t=1   2164  1168  4591\n"
     ]
    }
   ],
   "source": [
    "trainer.test_acsa(model, test_set, batch_size, device, best_th=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y, r_senti, ac_gold, ac_pred, w_senti, word_att, p_t, flag1, flag2 = trainer.output_attention(model, test_set, device, best_th=0.01551)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
